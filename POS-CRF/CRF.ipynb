{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "outputs": [],
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      #'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zZQQ0_GQZSeC"
      },
      "outputs": [],
      "source": [
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6MGQH5HjZSeG"
      },
      "outputs": [],
      "source": [
        "filepath = \"tamil_tagged_input.txt\"\n",
        "f = codecs.open(filepath, 'r', encoding='utf-8')\n",
        "file_contents = f.readlines()\n",
        "data=[]\n",
        "words=[]\n",
        "tags=[]\n",
        "for x in range(len(file_contents)):\n",
        "\tline = file_contents.pop(0).strip().split('\\\\')\n",
        "\tword,tag=line\n",
        "\tif word!='.':\n",
        "\t\twords.append(word)\n",
        "\t\ttags.append(tag)\n",
        "\telse:\n",
        "\t\twords.append(word)\n",
        "\t\ttags.append(tag)\n",
        "\t\tdata.append(tuple([words.copy(),tags.copy()]))\n",
        "\n",
        "\t\twords.clear()\n",
        "\t\ttags.clear()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOLpHlYZSeK",
        "outputId": "a7e36a07-4555-4b10-dc5c-31d04cb3f5ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7154"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "\n",
        "train_size = int(0.8*len(data))\n",
        "training = data[:train_size]\n",
        "testing = data[train_size:]\n",
        "X_train, y_train = transform_to_dataset(training)\n",
        "X_test, y_test = transform_to_dataset(testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy-Merm1gjWs",
        "outputId": "1ce9c494-0bd1-48b8-a6f7-0b025765c4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "[{'word': 'இல்\\xadலையா', 'is_first': True, 'is_last': False, 'prefix-1': 'இ', 'prefix-2': 'இல', 'prefix-3': 'இல்\\xad', 'suffix-1': 'ா', 'suffix-2': 'யா', 'suffix-3': 'லையா', 'prev_word': '', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': 'என்\\xadபதை', 'is_first': False, 'is_last': False, 'prefix-1': 'எ', 'prefix-2': 'என', 'prefix-3': 'என்\\xad', 'suffix-1': 'ை', 'suffix-2': 'தை', 'suffix-3': '\\xadபதை', 'prev_word': 'இல்\\xadலையா', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': 'மக்கள்', 'is_first': False, 'is_last': False, 'prefix-1': 'ம', 'prefix-2': 'மக', 'prefix-3': 'மக்க', 'suffix-1': '்', 'suffix-2': 'ள்', 'suffix-3': '்கள்', 'prev_word': 'என்\\xadபதை', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': 'முன்\\xadனி\\xadலையில்', 'is_first': False, 'is_last': False, 'prefix-1': 'ம', 'prefix-2': 'மு', 'prefix-3': 'முன்', 'suffix-1': '்', 'suffix-2': 'ல்', 'suffix-3': 'யில்', 'prev_word': 'மக்கள்', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': 'குறிப்\\xadபிட', 'is_first': False, 'is_last': False, 'prefix-1': 'க', 'prefix-2': 'கு', 'prefix-3': 'குறி', 'suffix-1': 'ட', 'suffix-2': 'ிட', 'suffix-3': '\\xadபிட', 'prev_word': 'முன்\\xadனி\\xadலையில்', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': 'வேண்டும்', 'is_first': False, 'is_last': False, 'prefix-1': 'வ', 'prefix-2': 'வே', 'prefix-3': 'வேண்', 'suffix-1': '்', 'suffix-2': 'ம்', 'suffix-3': 'டும்', 'prev_word': 'குறிப்\\xadபிட', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}, {'word': '.', 'is_first': False, 'is_last': True, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': 'வேண்டும்', 'next_word': '', 'has_hyphen': False, 'is_numeric': False}]\n",
            "<class 'list'>\n",
            "['NOUN', 'PROPN', 'NUM', 'NOUN', 'NOUN', 'ADV', 'PROPN', 'NOUN', 'NOUN', 'NOUN', 'ADJ', 'PROPN', 'PROPN', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'ADV', 'NOUN', 'NOUN', 'PUNCT']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(X_train))\n",
        "print(X_train[0])\n",
        "print(type(X_train[0]))\n",
        "print(y_test[0])\n",
        "print(type(X_test[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHTkotyWpd28",
        "outputId": "b321cecc-31fc-4581-feb7-c4a603aaf6aa"
      },
      "outputs": [],
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab.\n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are:\n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations (DUH!)\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "\n",
        "try:\n",
        "    model.fit(X_train, y_train)\n",
        "except AttributeError:\n",
        "    print(\"ok\")\n",
        "    pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('pos_crf_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlJwNkF_0zs",
        "outputId": "7e3f0709-002f-4f0d-bdf2-db0078e6f487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.8926912936590575\n",
            "F1 score on Training Data \n",
            "0.991623748381328\n"
          ]
        }
      ],
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_pred=model.predict(X_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_test, y_pred,average='weighted',labels=model.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_pred_train=model.predict(X_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_train, y_pred_train,average='weighted',labels=model.classes_))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
